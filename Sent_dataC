# Sentiment Analysis

#Sentiment lexicons
#There are several different sentiment lexicons available for sentiment analysis. You will explore three in this course that are available in the tidytext package:


# Load dplyr and tidytext
library(dplyr)
library(tidytext)

# Choose the bing lexicon to see what the "bing" lexicon looks like.
get_sentiments("bing")

# Choose the nrc lexicon
get_sentiments("nrc") %>%
  count(sentiment) # Count words by sentiment

#Implement an inner join
# geocoded_tweets 
geocoded_tweets

# Access bing lexicon: bing
bing <- get_sentiments("bing")

# Use data frame with text data
geocoded_tweets %>%
  # With inner join, implement sentiment analysis using `bing`
  inner_join(bing)

# What are the most common sadness words related to sadness in this Twitter dataset?
tweets_nrc

tweets_nrc %>%
  # Filter to only choose the words associated with sadness
  filter(sentiment == "sadness") %>%
  # Group by word
  group_by(word) %>%
  # Use the summarize verb to find the mean frequency
  summarize(freq = mean(freq)) %>%
  # Arrange to sort in order of descending frequency
  arrange(desc(freq))

#What are the most common joy words?
# tweets_nrc has been pre-defined
tweets_nrc

joy_words <- tweets_nrc %>%
  # Filter to choose only words associated with joy
  filter(sentiment =="joy") %>%
  # Group by each word
  group_by(word) %>%
  # Use the summarize verb to find the mean frequency
  summarize(freq = mean(freq)) %>%
  # Arrange to sort in order of descending frequency
  arrange(desc(freq) )   

# Load ggplot2
library(ggplot2)

joy_words %>%
  top_n(20) %>%
  mutate(word = reorder(word, freq)) %>%
  # Use aes() to put words on the x-axis and frequency on the y-axis
  ggplot(aes(word, freq)) +
  # Make a bar chart with geom_col()
  geom_col(stat = "identity") +
  coord_flip() 

# Do people in different states use different words?

tweets_nrc

tweets_nrc %>%
  # Find only the words for the state of Utah and associated with joy
  filter(state == "utah",
         sentiment == "joy") %>%
  # Arrange to sort in order of descending frequency
  arrange(desc(freq))

tweets_nrc %>%
  # Find only the words for the state of Louisiana and associated with joy
  filter(state == "louisiana",
         sentiment == "joy") %>%
  # Arrange to sort in order of descending frequency
  arrange(desc(freq))

# Which states have the most positive Twitter users?
# Combining your data with a sentiment lexicon, you can do all sorts of exploratory data analysis. 
tweets_bing

tweets_bing %>% 
  # Group by two columns: state and sentiment
  group_by(state, sentiment) %>%
  # Use summarize to calculate the mean frequency for these groups
  summarize(freq = mean(freq)) %>%
  spread(sentiment, freq) %>%
  ungroup() %>%
  # Calculate the ratio of positive to negative words
  mutate(ratio = positive / negative,
         state = reorder(state, ratio)) %>%
  # Use aes() to put state on the x-axis and ratio on the y-axis
  ggplot(aes(state, ratio)) +
  # Make a plot with points using geom_point()
  geom_point() +
  coord_flip()


# Work with Shekspeare data
# The data set shakespeare in available in the workspace
shakespeare

# Pipe the shakespeare data frame to the next line
shakespeare %>% 
  # Use count to find out how many titles/types there are
  count(title, type)
#Passing a dataset and a variable to count() returns the unique values in that variable and the corresponding count n, which in this case, is the number of lines in a play.


#Unnesting from text to word
#The shakespeare dataset is not yet compatible with tidy tools. You need to first break the text into individual tokens (the process of tokenization); a token is a meaningful unit of text for analysis, in many cases, just synonymous with a single word. You also need to transform the text to a tidy data structure with one token per row. You can use tidytext’s unnest_tokens() function to accomplish all of this at once.

## Load tidytext
library(tidytext)

tidy_shakespeare <- shakespeare %>%
  # Group by the titles of the plays
  group_by(title) %>%
  # Define a new column linenumber
  mutate(linenumber = row_number()) %>%
  # Transform the non-tidy text data to tidy text data
  unnest_tokens(word, text) %>%
  ungroup()

# Pipe the tidy shakespeare data frame to the next line
tidy_shakespeare %>% 
  # Use count to find out how many times each word is used
  count(word, sort = TRUE)

#Notice how the most common words in the data frame are words like “the”, “and”, and “i” that have no sentiments associated with them.Next, we'll join the data with a lexicon to implement sentiment analysis.
shakespeare_sentiment <- tidy_shakespeare %>%
  # Implement sentiment analysis with the "bing" lexicon
  inner_join(get_sentiments("bing")) 

shakespeare_sentiment %>%
  # Find how many positive/negative words each play has
  count(title, sentiment)
#Passing two variables to count() returns the count n for each unique combination of the two variables. In this case, you have 6 plays and 2 sentiments, so count() returns 6 x 2 = 12 rows.

shakespeare_sentiment %>%
  +     count(title, sentiment)

# make new column with mutate():

mutate(total = sum(n),
       percent = n / total)
# contributions by words  
hamlet_sentiment %>% 
  +     count(word, sentiment, sort = TRUE)

# Visualizing word contributions
# Looking at the percent column of output, we can see that tragedies do in fact have a higher percentage of negative words
sentiment_counts <- tidy_shakespeare %>%
  # Implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # Count the number of words by title, type, and sentiment
  count(title, type, sentiment)

sentiment_counts %>%
  # Group by the titles of the plays
  group_by(title) %>%
  # Find the total number of words in each play
  mutate(total = sum(n),
         # Calculate the number of words divided by the total
         percent = n/total ) %>%
  # Filter the results for only negative sentiment
  filter(sentiment == "negative") %>%
  arrange(percent)  

word_counts <- tidy_shakespeare %>%
  # Implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # Count by word and sentiment
  count(word, sentiment)

top_words <- word_counts %>%
  # Group by sentiment
  group_by(sentiment) %>%
  
  # Take the top 10 for each sentiment
  
  top_n(10) %>% 
  ungroup() %>%
  # Make word a factor in order of n
  mutate(word = reorder(word, n))

# Use aes() to put words on the x-axis and n on the y-axis
ggplot(top_words, aes(word, n, fill = sentiment)) +
  # Make a bar chart with geom_col()
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") +  
  coord_flip()
#  Death is pretty negative and love is positive, but are there words in that list that had different connotations during Shakespeare's time? Do you see a word that the lexicon has misidentified?  wilt

#  Back to the sentiment lexicons
> get_sentiments("bing") %>% 
  +     filter(word == "wilt")
library(stringr)
> shakespeare %>%
  +     filter(str_detect(text, "wilt")) %>%
  +     select(text) 
anti_join()

> tidy_shakespeare %>% 
  +     anti_join(data_frame(word = "wilt"))
Joining, by = "word"

#Word contributions by play
tidy_shakespeare %>%
  # Count by title and word
  count(title, word, sort = TRUE) %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Filter to only examine the scores for Macbeth that are negative
  filter(title == "The Tragedy of Macbeth", score < 0) 

# Calculating a contribution score
sentiment_contributions <- tidy_shakespeare %>%
  # Count by title and word
  count(title, word, sort = TRUE) %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn")) %>%
  # Group by title
  group_by(title) %>%
  # Calculate a contribution for each word in each title
  mutate(contribution = score* n/ sum(n)) %>%
  ungroup()

sentiment_contributions
#Notice that “hero” shows up in your results there; that is the name of one of the characters in “Much Ado About Nothing”.


sentiment_contributions %>%
  # Filter for Hamlet
  filter(title == "Hamlet, Prince of Denmark") %>%
  # Arrange to see the most negative words
  arrange(contribution)

sentiment_contributions %>%
  # Filter for The Merchant of Venice
  filter(title == "The Merchant of Venice") %>%
  # Arrange to see the most positive words
  arrange(desc(contribution))
# These are definitely characteristic words for these two plays.

# Which words are important in each play? Which words are important?
#  Tidy data principles make sentiment analysis easier and more effective. This is the first step in looking at narrative arcs.
tidy_shakespeare %>%
  # Implement sentiment analysis using "bing" lexicon
  inner_join(get_sentiments("bing")) %>%
  # Count using four arguments, the index makes chunks of text that are 70 lines long using integer division (%/%).
  count(title, type, index = linenumber %/%70, sentiment )


# Calculating net sentiment
# Load the tidyr package
library(tidyr)
# the net sentiment in each index-ed section of the play; net sentiment is the negative sentiment subtracted from the positive sentiment.    
tidy_shakespeare %>%
  inner_join(get_sentiments("bing")) %>%
  count(title, type, index = linenumber %/% 70, sentiment) %>%
  # Spread sentiment and n across multiple columns
  spread(sentiment, n, fill = 0) %>%
  # Using mutate to find net sentiment by subtracting negative sentiment from positive.
  mutate(sentiment = positive - negative)

# Visualizing narrative arcs
library(tidyr)
# Load the ggplot2 package
library(ggplot2)

tidy_shakespeare %>%
  inner_join(get_sentiments("bing")) %>%
  count(title, type, index = linenumber %/% 70, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  # Put index on x-axis, sentiment on y-axis, and map comedy/tragedy to fill
  ggplot( aes(index, sentiment, fill = type)) +
  # Make a bar chart with geom_col()
  geom_col(stat = "identity") + 
  # Separate panels for each title with facet_wrap()
  facet_wrap(~ title, scales = "free_x")    

##- - - - - - - -  - - - - - -  - - - - - -  - -  - - - - -  - - - - -  - - - - - - - - - - - - -
# Sentiment analysis of TV news
> climate_text %>%
  +     unnest_tokens(word, text)

# Load the tidytext package
library(tidytext)

# Pipe the climate_text dataset to the next line
tidy_tv <- climate_text %>%
  # Transform the non-tidy text data to tidy text data
  unnest_tokens(word, text)

# Counting totals.  find out what words are most common when discussing climate change on TV news, as well as the total number of words from each station. These are both helpful exploratory steps before moving on in analysis 
tidy_tv %>% 
  anti_join(stop_words) %>%
  # Count by word with sort = TRUE
  count(word, sort= TRUE)

tidy_tv %>%
  # Count by station  and it will make new colomn n
  count(station) %>%
  # Rename the new column n to  station_total
  rename(station_total= n)

# Sentiment analysis of TV news
tv_sentiment %>% 
  +     count(station, sentiment, station_total)
# Exploring contributions by word
> tv_sentiment %>% 
  +     count(sentiment, word)    

# Sentiment analysis of TV news
tv_sentiment <- tidy_tv %>% 
  # Group by station
  group_by(station) %>% 
  # Define a new column station_total
  mutate(station_total = n()) %>%
  ungroup() %>%
  # Implement sentiment analysis with the NRC lexicon
  inner_join(get_sentiments("nrc"))


# # Which stations use the most negative words?
tv_sentiment %>% 
  count(station, sentiment, station_total) %>%
  # Define a new column percent
  mutate(percent = n/station_total) %>%
  # Filter only for negative words
  filter(sentiment == "negative") %>%
  # Arrange by percent
  arrange(percent)

# Now do the same but for positive words
tv_sentiment %>% 
  count(station, sentiment, station_total) %>%
  mutate(percent = n/station_total) %>%
  filter(sentiment == "positive") %>%
  arrange(percent)

#Notice that MSNBC used a low proportion of negative words but a high proportion of positive words, the reverse is true of FOX News, and CNN is middle of the pack.

tv_sentiment %>%
  # Count by word and sentiment
  count(word, sentiment) %>%
  # Group by sentiment
  group_by(sentiment) %>%
  # Take the top 10 words for each sentiment
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  # Set up the plot with aes()
  ggplot( aes(word, n, fill  = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free") +
  coord_flip()

#Notice that you see proper names like Gore and Trump, which should be treated as neutral, and that “change” was a strong driver of fear sentiment, even though it is by definition part of these texts on climate change. It is important to see which words contribute to your sentiment scores so you can adjust the sentiment lexicons if appropriate.

# let'ssee which words contributed to which sentiment for this dataset of closed captioning texts about climate change from TV news station       
tv_sentiment %>%
  # Filter for only negative words
  filter(sentiment == "negative" ) %>%
  # Count by word and station
  count(word, station) %>%
  # Group by station
  group_by(station) %>%
  # Take the top 10 words for each station
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(paste(word, station, sep = "__"), n)) %>%
  # Set up the plot with aes()
  ggplot(aes(word, n, fill = station)) +
  geom_col(show.legend = FALSE) +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  facet_wrap(~ station, nrow = 2, scales = "free") +
  coord_flip()
# Some words, like “threat” are used by all three stations but some word choices are quite different. FOX News talks about terrorism and hurricanes, while CNN discusses hoaxes.    

# Using the lubridate package
#  For handling dates and times, try the lubridate package

floor_date(show_date, unit = "6 months")


# Visualizing sentiment over time.  how sentiment is changing over time. Are TV news stations using more negative words as time passes? More positive words?

library(lubridate)

sentiment_by_time <- tidy_tv %>%
  # Define a new column using floor_date()
  mutate(date = floor_date(show_date, unit = "6 months")) %>%
  # Group by date
  group_by(date) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  # Implement sentiment analysis using the NRC lexicon
  inner_join(get_sentiments("nrc"))

sentiment_by_time %>%
  # Filter for positive and negative words
  filter(sentiment %in% c("positive", "negative")) %>%
  # Count by date, sentiment, and total_words
  count(date, sentiment, total_words) %>%
  ungroup() %>%
  mutate(percent = n / total_words) %>%
  # Set up the plot with aes()
  ggplot(aes(date, percent, color = sentiment)) +
  geom_line(size = 1.5) +
  geom_smooth(method = "lm", se = FALSE, lty = 2) +
  expand_limits(y = 0) 
# conclusion: The proportion of positive words looks flat, and the proportion of negative words may be increasing.




# we can also use tidy data principles to explore how individual words have been used over time    
tidy_tv %>%
  # Define a new column that rounds each date to the nearest 1 month
  mutate( date = floor_date(show_date, unit = "1 month") ) %>%
  filter(word %in% c("threat", "hoax", "denier",
                     "real", "warming", "hurricane")) %>%
  # Count by date and word
  count(date, word) %>%
  ungroup() %>%
  # Set up your plot with aes()
  ggplot(aes(date, n, color = word)) +
  # Make facets by word
  facet_wrap(~word) +
  geom_line(size = 1.5, show.legend = FALSE) +
  expand_limits(y = 0)
# conclusion: You can see that words like “hoax” and “denier” have been used only recently, and “warming” is decreasing in monthly uses. You can see when a hurricane was being discussed as well.

#  4 Ranking pop songs through the years- - - - - - - - - - - - - - - - - - -  - - - - - - - - - - -
#Tidying song lyrics
#   > tidy_lyrics %>%
#     +     count(word, sort = TRUE)
# #Sentiment analysis of pop songs
#   > lyric_sentiment %>%
#     +     count(song, sentiment, total_words)  

# Load the tidytext package
library(tidytext)

# Pipe song_lyrics to the next line
tidy_lyrics <- song_lyrics %>% 
  # Transform the lyrics column to a word column
  unnest_tokens(word, lyrics)

# Print tidy_lyrics
tidy_lyrics

#  the total number of words for each song    
totals <- tidy_lyrics %>%
  # Count by song to find the word totals for each song
  count(song) %>%
  # Rename the new column
  rename(total_words = n)

# Print totals    
totals
# Sentiment analysis on song lyrics. NRC lexicon has 10 categories of sentiment:
#   anger,  anticipation, disgust, fear,  joy,  negative,  positive,  sadness,  surprise,  trust

lyric_counts <- tidy_lyrics %>%
  # Combine totals with tidy_lyrics using the "song" column
  left_join(totals, by = "song")

lyric_sentiment <- lyric_counts %>%
  # Implement sentiment analysis with the "nrc" lexicon
  inner_join(get_sentiments("nrc"))

lyric_sentiment %>%
  # Find how many sentiment words each song has
  count(song,sentiment, sort = TRUE)

# The most positive and negative songs    
## What songs have the highest proportion of negative words?
lyric_sentiment %>%
  # Count using three arguments
  count(song,  sentiment, total_words) %>%
  ungroup() %>%
  # Make a new percent column with mutate 
  mutate(percent = n/total_words ) %>%
  # Filter for only negative words
  filter(sentiment == "negative") %>%
  # Arrange by descending percent
  arrange(desc(percent))

# What songs have the highest proportion of positive words?
lyric_sentiment %>%
  count(song, sentiment, total_words) %>%
  ungroup() %>%
  mutate(percent = n/total_words) %>%
  filter(sentiment == "positive") %>%
  arrange(desc(percent))
# conclusion : One of the songs with the highest proportion of positive words is “Dance, Dance, Dance (Yowsah, Yowsah, Yowsah)” from 1977.    


# Sentiment and rank
> lyric_sentiment %>%
  +     filter(sentiment == "positive") %>%
  +     count(song, rank, total_words)    

# Sentiment and Billboard rank
lyric_sentiment %>%
  filter(sentiment == "positive") %>%
  # Count by song, Billboard rank, and the total number of words
  count(song, rank, total_words) %>%
  ungroup() %>%
  # Use the correct dplyr verb to make two new columns
  mutate(percent = n / total_words,
         rank = 10 * floor(rank / 10)) %>%
  ggplot(aes(as.factor(rank), percent)) +
  # Make a boxplot
  geom_boxplot()   

lyric_sentiment %>%
  # Filter for only negative words
  filter(sentiment =="negative") %>%
  # Count by song, Billboard rank, and the total number of words
  count(song, rank, total_words) %>%
  ungroup() %>%
  # Mutate to make a percent column
  mutate(percent = n/total_words,
         rank = 10 * floor(rank / 10)) %>%
  # Use ggplot to set up a plot with rank and percent
  ggplot(aes(as.factor(rank), percent, fill = rank)) +
  # Make a boxplot
  geom_boxplot()
# conclusion: Sentiment and Billboard rank appear to be unrelated.

#Moving from song rank to year

#Pop songs over time
> lyric_sentiment %>%
  +     filter(sentiment == "positive") %>%
  +     count(song, year, total_words)
# Modeling sentiment
> sentiment_model <- lm(percent ~ year, data = sentiment_by_year)

> summary(sentiment_model)

# How is negative sentiment changing over time?
lyric_sentiment %>%
  # Filter for only negative words
  filter(sentiment =="negative") %>%
  # Count by song, year, and the total number of words
  count(song, year, total_words) %>%
  ungroup() %>%
  mutate(percent = n / total_words,
         year = 10 * floor(year / 10)) %>%
  # Use ggplot to set up a plot with year and percent
  ggplot(aes(as.factor(year), percent)) +
  geom_boxplot()

# How is positive sentiment changing over time?
lyric_sentiment %>%
  filter(sentiment == "positive") %>%
  count(song, year, total_words) %>%
  ungroup() %>%
  mutate(percent = n / total_words,
         year = 10 * floor(year / 10)) %>%
  ggplot(aes(as.factor(year), percent)) +
  geom_boxplot()
# conclusion: Notice that the proportion of negative words does not change significantly but the proportion of positive words exhibits a decrease over time.

# Modeling negative sentiment
negative_by_year <- lyric_sentiment %>%
  # Filter for negative words
  filter(sentiment=="negative") %>%
  count(song, year, total_words) %>%
  ungroup() %>%
  # Define a new column: percent
  mutate(percent = n/total_words)

# Specify the model with percent as the response and year as the predictor
model_negative <- lm(percent ~ year, data = negative_by_year)

# Use summary to see the results of the model fitting
summary(model_negative)

# conclusion Notice how high the p-values are; negative sentiment does not change significantly with year.
positive_by_year <- lyric_sentiment %>%
  filter(sentiment == "positive") %>%
  # Count by song, year, and total number of words
  count(song, year, total_words) %>%
  ungroup() %>%
  # Define a new column: percent
  mutate(percent = n/total_words)

# Fit a linear model with percent as the response and year as the predictor
model_positive <- lm(percent~ year, positive_by_year)

# Use summary to see the results of the model fitting
summary(model_positive)
